# LLM 

## Attention

### FlashAttention

- FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
- FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
- FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision

### Other Optimization

- Fast Transformer Decoding: One Write-Head is All You Need (MQA)
- GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
- Efficient Memory Management for Large Language Model Serving with  PagedAttention (SOSP '23)

## Speculative Decoding

- SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification (ASPLOS '24)

## Library

- [FlashInfer](https://github.com/flashinfer-ai/flashinfer.git)